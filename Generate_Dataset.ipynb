{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d541785-176b-4757-b2ac-8562264250da",
   "metadata": {},
   "source": [
    "# Script to Extract Eurorack Dataset from ModularGrid\n",
    "\n",
    "<br>\n",
    "\n",
    "Uses Selenium and BeautifulSoup to scrape ModularGrid and generate the dataset. It can take a considerable amount of time to retrieve a large dataset. \n",
    "\n",
    "<br>\n",
    "\n",
    "- <code>get_module_links</code> get links from modulargrid.net main eurorack browser page. \n",
    "    Pass browser URL, number of pages to load (1 page = 40 modules), and a short delay to load the page as webdriver scrolls (1.5 - 2 secs). \n",
    "    Returns list of links to module pages.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- <code>get_module_data</code> visit each module page grabbed by <code>get_module_links</code>. Pass list of links. \n",
    "    Returns pandas dataframe containing module information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf661b0-27d6-49e3-b6a6-68dc837bb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b79ad2-b8db-4d16-ba54-e9ba2515b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_links(url, pages_to_load, scroll_pause_time):\n",
    "    #run headless chrome\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    #delay to load browser page\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #list of links\n",
    "    links = []\n",
    "    \n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for i in range(0, pages_to_load):\n",
    "    \n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        # Wait to load page\n",
    "        time.sleep(scroll_pause_time)\n",
    "    \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "        #Progress counter\n",
    "        clear_output(wait=True)\n",
    "        print(\"Pages Loaded: \", i+1)\n",
    "\n",
    "    #list of links to scrape\n",
    "    links = []\n",
    "    \n",
    "    #get loaded modules link list\n",
    "    modules = driver.find_elements(By.CLASS_NAME, 'box-module')\n",
    "    for module in modules:\n",
    "         link = module.find_element(By.TAG_NAME, 'a')\n",
    "         links.append(link.get_attribute('href'))\n",
    "    \n",
    "    # close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    #amount of module links\n",
    "    print('Module Links Acquired:', len(links))\n",
    "    \n",
    "    return links    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c06e4-0ba9-4cf9-ac42-bb2c759b2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_data(links):\n",
    "    #takes list of links to scape\n",
    "    #gets data using soup, faster than selenium\n",
    "    \n",
    "    data_dict_list = []\n",
    "\n",
    "    #progress Counter\n",
    "    n = 1\n",
    "    \n",
    "    #iterate through links and get data\n",
    "    for link in links:\n",
    "        try:\n",
    "            page = requests.get(link) #headers=headers\n",
    "            soup = BeautifulSoup(page.text, 'html')\n",
    "            \n",
    "            #create list of functions\n",
    "            functions_html = soup.find_all('span', class_='label label-info')\n",
    "            functions = [label.text for label in functions_html]\n",
    "    \n",
    "            #name\n",
    "            name = soup.find('h1').text.strip()\n",
    "    \n",
    "            #manufacturer\n",
    "            manufacturer = soup.find('span', class_='vendor-name').text.strip()\n",
    "    \n",
    "            #hp\n",
    "            hp_html = soup.find_all('dd')[0]\n",
    "            if hp_html:\n",
    "                hp = int(hp_html.text.strip()[:2])\n",
    "            else:\n",
    "                hp = np.nan\n",
    "        \n",
    "            #depth\n",
    "            depth_mm = soup.find_all('dd')[1].text.strip()[3:5]\n",
    "            if depth_mm == 'mm':\n",
    "                depth = int(soup.find_all('dd')[1].text.strip()[:2])\n",
    "            else:\n",
    "                depth = np.nan\n",
    "    \n",
    "            #power\n",
    "            power_html = soup.find_all('dl')[1]\n",
    "            try:\n",
    "                pos12_html = power_html.find_all('dd')[0]\n",
    "                try:\n",
    "                    pos12 = float(pos12_html.text.strip().split()[0]) \n",
    "                except ValueError:\n",
    "                    pos12 = np.nan\n",
    "            except IndexError:\n",
    "                pos12 = np.nan\n",
    "            \n",
    "            try:\n",
    "                neg12_html = power_html.find_all('dd')[1]\n",
    "                try:\n",
    "                    neg12 = float(neg12_html.text.strip().split()[0]) \n",
    "                except ValueError:\n",
    "                    neg12 = np.nan\n",
    "            except IndexError:\n",
    "                neg12 = np.nan\n",
    "            \n",
    "            try:\n",
    "                pos5_html = power_html.find_all('dd')[2]\n",
    "                try:\n",
    "                    pos5 = float(pos5_html.text.strip().split()[0]) \n",
    "                except ValueError:\n",
    "                    pos5 = np.nan\n",
    "            except IndexError:\n",
    "                pos5 = np.nan\n",
    "    \n",
    "            \n",
    "            #price\n",
    "            price_html = soup.find('span', class_='price')\n",
    "            if price_html:\n",
    "                price = float(price_html.text.strip()[1:])\n",
    "            else:\n",
    "                price = np.nan\n",
    "    \n",
    "            #racks\n",
    "            racks_html = soup.find('div', id='related-racks')\n",
    "            racks_html = racks_html.find('strong')\n",
    "            if racks_html:\n",
    "                racks = int(racks_html.text.strip())\n",
    "            else:\n",
    "                racks = np.nan\n",
    "        \n",
    "            #rating\n",
    "            rating_data_html = soup.find('div', class_='g-descr').find_all('span')\n",
    "            try:\n",
    "                rating_html = rating_data_html[-2]\n",
    "                votes_html = rating_data_html[-1]\n",
    "                rating = float(rating_html.text.strip())\n",
    "                votes = int(votes_html.text.strip())\n",
    "            except IndexError:\n",
    "                rating = np.nan\n",
    "                votes = np.nan\n",
    "            except ValueError:\n",
    "                rating = np.nan\n",
    "                vote = np.nan\n",
    "    \n",
    "            #available\n",
    "            if soup.find('p', class_='text-success'):\n",
    "                available = 'Available'\n",
    "            elif soup.find('p', class_='text-error'):\n",
    "                available = 'Discontinued'\n",
    "            else:\n",
    "                available = np.nan\n",
    "    \n",
    "            #approved\n",
    "            approved_html = soup.find('div', class_='box-approved')\n",
    "            if approved_html:\n",
    "                approved = 1\n",
    "            else:\n",
    "                approved = 0\n",
    "            \n",
    "            data_dict = {\n",
    "                'Name': name,\n",
    "                'Manufacturer': manufacturer,\n",
    "                'Functions': functions,\n",
    "                'HP': hp,\n",
    "                'Depth': depth,\n",
    "                '+12V (mA)': pos12,\n",
    "                '-12V (mA)': neg12,\n",
    "                '+5V (mA)': pos5,\n",
    "                'Price (â‚¬)': price,\n",
    "                'Racks': racks,\n",
    "                'Rating': rating,\n",
    "                'Votes': votes,\n",
    "                'Available': available,\n",
    "                'Approved': approved\n",
    "            }\n",
    "    \n",
    "            #add dict to list of dicts\n",
    "            data_dict_list.append(data_dict)\n",
    "    \n",
    "            clear_output(wait=True)\n",
    "            print('Modules Scraped:', n, '|', manufacturer, '-', name)\n",
    "            n += 1\n",
    "\n",
    "        #if something fatal happens break loop so df is created\n",
    "        except IndexError:\n",
    "            break\n",
    "        except ValueError:\n",
    "            break\n",
    "    \n",
    "    #create dataframe from list of dicts\n",
    "    df = pd.DataFrame(data_dict_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c17d01-0238-4b57-b38d-2eca70d7ccaf",
   "metadata": {},
   "source": [
    "- <code>URL</code> target link to ModularGrid main browser.\n",
    "\n",
    "- <code>N</code> intended length of dataset.\n",
    "\n",
    "- <code>YEAR</code> year the dataset was generated.\n",
    "\n",
    "- <code>PAGES_TO_LOAD</code> number of pages to load (1 page = 40 modules).\n",
    "\n",
    "- <code>SCROLL_PAUSE_TIME</code> a short delay to load the page as webdriver scrolls (1.5 - 2 secs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfd218-d4cb-4554-818d-265854dbdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target url, modulargrid.net - browser - sort by popularity - ascending - only full size modules (no 1U)\n",
    "URL = 'https://modulargrid.net/e/modules/browser?SearchName=&SearchVendor=&SearchFunction=&SearchSecondaryfunction=&SearchHeight=f&SearchTe=&SearchTemethod=max&SearchBuildtype=&SearchLifecycle=&SearchSet=&SearchMarketplace=&SearchIsmodeled=0&SearchShowothers=0&order=popular&direction=asc'\n",
    "\n",
    "#max length of dataset\n",
    "#make sure enough pages are loaded to contain enough modules\n",
    "N = 40\n",
    "YEAR = 2024\n",
    "\n",
    "#the amount of pages to load, each page is 40 modules ish\n",
    "PAGES_TO_LOAD = 1\n",
    "    \n",
    "#give time for new page to load, 1.5 - 2 secs\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "#get links to each module from browser page\n",
    "links = get_module_links(URL, PAGES_TO_LOAD, SCROLL_PAUSE_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060aeb70-af29-4366-ad68-13a00695e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list n elements long\n",
    "\n",
    "#links = links[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237825f-13f6-45fe-96ff-1b3229d8daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from module links\n",
    "df = get_module_data(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ac3c7-2a3f-48a9-9c8f-aa6168e3d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adc0d7-9148-4ccc-b5b5-b6e0c8ff5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv file on hardrive\n",
    "\n",
    "#df.to_csv(f\"{N}_most_popular_eurorack_modules_{YEAR}_UNCLEANED.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282aa3d9-dc6c-4d44-885b-94fbdd4d3cbe",
   "metadata": {},
   "source": [
    "## Wes Leggo-Morrell 2024\n",
    "\n",
    "#### **[Instagram](https://www.instagram.com/modular.mooch)**\n",
    "\n",
    "#### **[GitHub](https://github.com/WesDaMooch)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
